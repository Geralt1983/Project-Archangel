diff --git a/app/api.py b/app/api.py
index 819e2c1..38249ff 100644
--- a/app/api.py
+++ b/app/api.py
@@ -106,6 +106,8 @@ async def todoist_webhook(request: Request, x_todoist_hmac_sha256: str = Header(
 @app.post("/tasks/intake")
 async def intake(task: dict, provider: str = Query("clickup")):
     from app.orchestrator import create_orchestrator, TaskContext, TaskState
+    from app.utils.outbox import OutboxManager, create_task_operation
+    from app.db_pg import _ensure_conn
     
     adapter = get_adapter(provider)
     t = triage_with_serena(task, provider=adapter.name)
@@ -149,18 +151,51 @@ async def intake(task: dict, provider: str = Query("clickup")):
         }
     }
     
-    created = adapter.create_task(t)
-    external_id = created.get("id")
-    if t.get("subtasks"):
-        adapter.create_subtasks(external_id, t["subtasks"])
-    if t.get("checklist"):
-        adapter.add_checklist(external_id, t["checklist"])
-    t["external_id"] = external_id
-    t["provider"] = adapter.name
-    save_task(t)
-    if external_id:
-        map_upsert(adapter.name, external_id, t["id"])
-    log_event("pushed", {"task_id": t["id"], "external_id": external_id, "provider": adapter.name})
+    # Use outbox pattern for reliable task creation
+    outbox = OutboxManager(_ensure_conn)
+    
+    try:
+        # Immediate provider operation with retry
+        created = adapter.create_task(t)
+        external_id = created.get("id")
+        
+        # Create outbox operations for subtasks and checklist (async operations)
+        if t.get("subtasks"):
+            outbox.add_operation(
+                operation_type="create_subtasks",
+                provider=adapter.name,
+                endpoint=f"/tasks/{external_id}/subtasks",
+                payload={"parent_id": external_id, "subtasks": t["subtasks"]},
+                metadata={"task_id": t["id"]}
+            )
+        
+        if t.get("checklist"):
+            outbox.add_operation(
+                operation_type="add_checklist", 
+                provider=adapter.name,
+                endpoint=f"/tasks/{external_id}/checklist",
+                payload={"task_id": external_id, "items": t["checklist"]},
+                metadata={"task_id": t["id"]}
+            )
+        
+        t["external_id"] = external_id
+        t["provider"] = adapter.name
+        save_task(t)
+        if external_id:
+            map_upsert(adapter.name, external_id, t["id"])
+        log_event("pushed", {"task_id": t["id"], "external_id": external_id, "provider": adapter.name})
+        
+    except Exception as e:
+        # If immediate creation fails, add to outbox for retry
+        log_event("outbox_fallback", {"task_id": t["id"], "error": str(e), "provider": adapter.name})
+        
+        outbox_op = create_task_operation(adapter.name, t, outbox)
+        t["external_id"] = None
+        t["provider"] = adapter.name
+        t["outbox_operation_id"] = outbox_op.id
+        save_task(t)
+        
+        external_id = None
     return {
         "id": t["id"], "provider": adapter.name, "external_id": external_id,
         "status": "triaged", "score": t["score"],
@@ -412,4 +447,74 @@ async def provider_health_check():
         "health_check": health_results,
         "provider_stats": provider_stats,
         "registered_providers": list(manager.providers.keys())
+    }
+
+# Outbox Management Endpoints
+@app.get("/outbox/stats")
+async def outbox_stats():
+    """Get outbox operation statistics"""
+    from app.utils.outbox import OutboxManager
+    from app.db_pg import _ensure_conn
+    
+    outbox = OutboxManager(_ensure_conn)
+    stats = outbox.get_stats()
+    
+    return {
+        "outbox_stats": stats,
+        "total_operations": sum(stats.values())
+    }
+
+@app.post("/outbox/process")
+async def process_outbox(batch_size: int = 50):
+    """Process pending outbox operations"""
+    from app.utils.outbox import OutboxManager, OutboxProcessor
+    from app.db_pg import _ensure_conn
+    
+    # Create provider registry for processor
+    provider_registry = {
+        "clickup": get_adapter("clickup"),
+        # Add other providers as needed
+    }
+    
+    outbox = OutboxManager(_ensure_conn)
+    processor = OutboxProcessor(outbox, provider_registry)
+    
+    try:
+        stats = await processor.process_pending(batch_size)
+        return {
+            "processing_stats": stats,
+            "success": True
+        }
+    except Exception as e:
+        return {
+            "error": str(e),
+            "success": False
+        }
+
+@app.get("/outbox/pending")
+async def get_pending_operations(limit: int = 20):
+    """Get pending outbox operations"""
+    from app.utils.outbox import OutboxManager
+    from app.db_pg import _ensure_conn
+    
+    outbox = OutboxManager(_ensure_conn)
+    operations = outbox.get_pending_operations(limit)
+    
+    return {
+        "pending_operations": [op.to_dict() for op in operations],
+        "count": len(operations)
+    }
+
+@app.post("/outbox/cleanup")
+async def cleanup_outbox(older_than_days: int = 7):
+    """Clean up completed outbox operations"""
+    from app.utils.outbox import OutboxManager
+    from app.db_pg import _ensure_conn
+    
+    outbox = OutboxManager(_ensure_conn)
+    cleaned_count = outbox.cleanup_completed(older_than_days)
+    
+    return {
+        "cleaned_operations": cleaned_count,
+        "older_than_days": older_than_days
     }
\ No newline at end of file
diff --git a/app/providers/clickup.py b/app/providers/clickup.py
index ee85b4a..a140c47 100644
--- a/app/providers/clickup.py
+++ b/app/providers/clickup.py
@@ -1,6 +1,7 @@
 import os, hmac, hashlib, httpx
 from datetime import datetime, timezone
 from .base import ProviderAdapter
+from ..utils.retry import retry_with_backoff, RetryConfig, RateLimitError, ServerError
 
 CLICKUP_API = "https://api.clickup.com/api/v2"
 
@@ -18,7 +19,17 @@ class ClickUpAdapter(ProviderAdapter):
         self.list_id = list_id
         self.webhook_secret = webhook_secret
         self.client = httpx.Client(timeout=20.0, headers={"Authorization": token})
+        
+        # Enhanced retry configuration for ClickUp
+        self.retry_config = RetryConfig(
+            max_attempts=5,
+            base_delay=1.0,
+            max_delay=60.0,
+            jitter=True,
+            retryable_exceptions=(RateLimitError, ServerError, httpx.RequestError, httpx.TimeoutException)
+        )
 
+    @retry_with_backoff()  # Use default config from decorator
     def create_task(self, task):
         payload = {
             "name": task["title"],
@@ -26,10 +37,27 @@ class ClickUpAdapter(ProviderAdapter):
             "due_date": _to_epoch_ms(task.get("deadline")),
             "tags": task.get("labels", []),
         }
-        r = self.client.post(f"{CLICKUP_API}/list/{self.list_id}/task", json=payload)
-        r.raise_for_status()
+        r = self._make_request("POST", f"{CLICKUP_API}/list/{self.list_id}/task", json=payload)
         return r.json()
+    
+    def _make_request(self, method: str, url: str, **kwargs):
+        """Centralized request method with error handling"""
+        response = self.client.request(method, url, **kwargs)
+        
+        # Handle rate limiting
+        if response.status_code == 429:
+            retry_after = int(response.headers.get("retry-after", 60))
+            raise RateLimitError(retry_after)
+        
+        # Handle server errors
+        if response.status_code >= 500:
+            raise ServerError(response.status_code, response.text)
+        
+        # Handle client errors (don't retry these)
+        response.raise_for_status()
+        return response
 
+    @retry_with_backoff()
     def create_subtasks(self, parent_external_id, subtasks):
         out = []
         for st in subtasks:
@@ -37,23 +65,21 @@ class ClickUpAdapter(ProviderAdapter):
                 "name": st["title"],
                 "parent": parent_external_id
             }
-            r = self.client.post(f"{CLICKUP_API}/list/{self.list_id}/task", json=payload)
-            if r.status_code == 429:
-                self._backoff(r)
-                r = self.client.post(f"{CLICKUP_API}/list/{self.list_id}/task", json=payload)
-            r.raise_for_status()
+            r = self._make_request("POST", f"{CLICKUP_API}/list/{self.list_id}/task", json=payload)
             out.append(r.json())
         return out
 
+    @retry_with_backoff()
     def add_checklist(self, external_id, items):
         # ClickUp supports checklists on tasks
         for it in items:
             # Create a checklist with a single item name
             # If you prefer one checklist with many items, first create checklist then items
-            self.client.post(f"{CLICKUP_API}/task/{external_id}/checklist", json={"name": it})
+            self._make_request("POST", f"{CLICKUP_API}/task/{external_id}/checklist", json={"name": it})
 
+    @retry_with_backoff()
     def update_status(self, external_id, status):
-        self.client.put(f"{CLICKUP_API}/task/{external_id}", json={"status": status})
+        self._make_request("PUT", f"{CLICKUP_API}/task/{external_id}", json={"status": status})
 
     def verify_webhook(self, headers, raw_body):
         # ClickUp sends X Signature header with HMAC SHA256 hex of raw body using webhook secret
@@ -62,17 +88,12 @@ class ClickUpAdapter(ProviderAdapter):
         mac = hmac.new(self.webhook_secret.encode(), raw_body, hashlib.sha256).hexdigest()
         return hmac.compare_digest(sig, mac)
 
+    @retry_with_backoff()
     def create_webhook(self, callback_url: str):
         payload = {
             "endpoint": callback_url,
             "events": ["taskCreated", "taskUpdated", "taskDeleted"],
             "secret": self.webhook_secret
         }
-        r = self.client.post(f"{CLICKUP_API}/team/{self.team_id}/webhook", json=payload)
-        r.raise_for_status()
-        return r.json()
-
-    def _backoff(self, resp):
-        import time, random
-        retry_after = float(resp.headers.get("Retry-After", "1"))
-        time.sleep(min(5.0, retry_after) + random.random())
\ No newline at end of file
+        r = self._make_request("POST", f"{CLICKUP_API}/team/{self.team_id}/webhook", json=payload)
+        return r.json()
\ No newline at end of file
diff --git a/app/utils/outbox.py b/app/utils/outbox.py
new file mode 100644
index 0000000..a8a0b1e
--- /dev/null
+++ b/app/utils/outbox.py
@@ -0,0 +1,448 @@
+"""
+Outbox Pattern for reliable provider operations with idempotency
+"""
+
+import hashlib
+import json
+import uuid
+from datetime import datetime, timezone, timedelta
+from typing import Dict, Any, Optional, List
+from enum import Enum
+import logging
+
+logger = logging.getLogger(__name__)
+
+class OutboxStatus(Enum):
+    PENDING = "pending"
+    PROCESSING = "processing"
+    COMPLETED = "completed"
+    FAILED = "failed"
+    CANCELLED = "cancelled"
+
+class OutboxOperation:
+    """Represents an outbound operation to be performed"""
+    
+    def __init__(
+        self,
+        operation_type: str,
+        provider: str,
+        endpoint: str,
+        payload: Dict[str, Any],
+        idempotency_key: Optional[str] = None,
+        retry_count: int = 0,
+        max_retries: int = 5,
+        metadata: Optional[Dict[str, Any]] = None
+    ):
+        self.id = str(uuid.uuid4())
+        self.operation_type = operation_type
+        self.provider = provider
+        self.endpoint = endpoint
+        self.payload = payload
+        self.idempotency_key = idempotency_key or self._generate_idempotency_key()
+        self.status = OutboxStatus.PENDING
+        self.retry_count = retry_count
+        self.max_retries = max_retries
+        self.created_at = datetime.now(timezone.utc)
+        self.updated_at = self.created_at
+        self.next_retry_at = None
+        self.error_message = None
+        self.metadata = metadata or {}
+    
+    def _generate_idempotency_key(self) -> str:
+        """Generate idempotency key from provider, endpoint, and payload hash"""
+        payload_str = json.dumps(self.payload, sort_keys=True)
+        combined = f"{self.provider}|{self.endpoint}|{payload_str}"
+        return hashlib.sha256(combined.encode()).hexdigest()[:32]
+    
+    def to_dict(self) -> Dict[str, Any]:
+        """Convert to dictionary for storage"""
+        return {
+            'id': self.id,
+            'operation_type': self.operation_type,
+            'provider': self.provider,
+            'endpoint': self.endpoint,
+            'payload': self.payload,
+            'idempotency_key': self.idempotency_key,
+            'status': self.status.value,
+            'retry_count': self.retry_count,
+            'max_retries': self.max_retries,
+            'created_at': self.created_at.isoformat(),
+            'updated_at': self.updated_at.isoformat(),
+            'next_retry_at': self.next_retry_at.isoformat() if self.next_retry_at else None,
+            'error_message': self.error_message,
+            'metadata': self.metadata
+        }
+    
+    @classmethod
+    def from_dict(cls, data: Dict[str, Any]) -> 'OutboxOperation':
+        """Create from dictionary"""
+        op = cls(
+            operation_type=data['operation_type'],
+            provider=data['provider'],
+            endpoint=data['endpoint'],
+            payload=data['payload'],
+            idempotency_key=data['idempotency_key'],
+            retry_count=data['retry_count'],
+            max_retries=data['max_retries'],
+            metadata=data.get('metadata', {})
+        )
+        op.id = data['id']
+        op.status = OutboxStatus(data['status'])
+        op.created_at = datetime.fromisoformat(data['created_at'])
+        op.updated_at = datetime.fromisoformat(data['updated_at'])
+        op.next_retry_at = datetime.fromisoformat(data['next_retry_at']) if data['next_retry_at'] else None
+        op.error_message = data.get('error_message')
+        return op
+
+class OutboxManager:
+    """Manages outbox operations with database persistence"""
+    
+    def __init__(self, db_connection_func):
+        self.get_db = db_connection_func
+        self._init_schema()
+    
+    def _init_schema(self):
+        """Initialize outbox table schema"""
+        conn = self.get_db()
+        cursor = conn.cursor()
+        try:
+            cursor.execute("""
+                CREATE TABLE IF NOT EXISTS outbox_operations (
+                    id TEXT PRIMARY KEY,
+                    operation_type TEXT NOT NULL,
+                    provider TEXT NOT NULL,
+                    endpoint TEXT NOT NULL,
+                    payload JSONB NOT NULL,
+                    idempotency_key TEXT NOT NULL,
+                    status TEXT NOT NULL,
+                    retry_count INTEGER NOT NULL DEFAULT 0,
+                    max_retries INTEGER NOT NULL DEFAULT 5,
+                    created_at TIMESTAMPTZ NOT NULL,
+                    updated_at TIMESTAMPTZ NOT NULL,
+                    next_retry_at TIMESTAMPTZ,
+                    error_message TEXT,
+                    metadata JSONB,
+                    UNIQUE(idempotency_key)
+                );
+                
+                CREATE INDEX IF NOT EXISTS idx_outbox_status_retry 
+                ON outbox_operations(status, next_retry_at);
+                
+                CREATE INDEX IF NOT EXISTS idx_outbox_provider 
+                ON outbox_operations(provider);
+                
+                CREATE INDEX IF NOT EXISTS idx_outbox_created 
+                ON outbox_operations(created_at);
+            """)
+            conn.commit()
+        finally:
+            cursor.close()
+    
+    def add_operation(
+        self,
+        operation_type: str,
+        provider: str,
+        endpoint: str,
+        payload: Dict[str, Any],
+        idempotency_key: Optional[str] = None,
+        max_retries: int = 5,
+        metadata: Optional[Dict[str, Any]] = None
+    ) -> OutboxOperation:
+        """Add new operation to outbox"""
+        
+        operation = OutboxOperation(
+            operation_type=operation_type,
+            provider=provider,
+            endpoint=endpoint,
+            payload=payload,
+            idempotency_key=idempotency_key,
+            max_retries=max_retries,
+            metadata=metadata
+        )
+        
+        conn = self.get_db()
+        cursor = conn.cursor()
+        try:
+            # Adapt SQL for SQLite vs PostgreSQL
+            if hasattr(conn, 'row_factory'):  # SQLite
+                cursor.execute("""
+                    INSERT OR IGNORE INTO outbox_operations (
+                        id, operation_type, provider, endpoint, payload,
+                        idempotency_key, status, retry_count, max_retries,
+                        created_at, updated_at, metadata
+                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
+                """, (
+                    operation.id, operation.operation_type, operation.provider,
+                    operation.endpoint, json.dumps(operation.payload),
+                    operation.idempotency_key, operation.status.value,
+                    operation.retry_count, operation.max_retries,
+                    operation.created_at.isoformat(), operation.updated_at.isoformat(),
+                    json.dumps(operation.metadata)
+                ))
+            else:  # PostgreSQL
+                cursor.execute("""
+                    INSERT INTO outbox_operations (
+                        id, operation_type, provider, endpoint, payload,
+                        idempotency_key, status, retry_count, max_retries,
+                        created_at, updated_at, metadata
+                    ) VALUES (%s, %s, %s, %s, %s::jsonb, %s, %s, %s, %s, %s, %s, %s::jsonb)
+                    ON CONFLICT (idempotency_key) DO NOTHING
+                """, (
+                    operation.id, operation.operation_type, operation.provider,
+                    operation.endpoint, json.dumps(operation.payload),
+                    operation.idempotency_key, operation.status.value,
+                    operation.retry_count, operation.max_retries,
+                    operation.created_at, operation.updated_at,
+                    json.dumps(operation.metadata)
+                ))
+                
+            # Check if operation was actually inserted (idempotency)
+            if hasattr(conn, 'row_factory'):  # SQLite
+                cursor.execute("SELECT id FROM outbox_operations WHERE idempotency_key = ?", (operation.idempotency_key,))
+            else:  # PostgreSQL
+                cursor.execute("SELECT id FROM outbox_operations WHERE idempotency_key = %s", (operation.idempotency_key,))
+            
+            result = cursor.fetchone()
+            
+            if result and result[0] != operation.id:
+                # Operation already exists with this idempotency key
+                logger.info(f"Operation with idempotency key {operation.idempotency_key} already exists")
+                return self.get_operation_by_key(operation.idempotency_key)
+            
+            conn.commit()
+        
+        except Exception as e:
+            logger.error(f"Failed to add outbox operation: {e}")
+            raise
+        finally:
+            cursor.close()
+        
+        return operation
+    
+    def get_operation_by_key(self, idempotency_key: str) -> Optional[OutboxOperation]:
+        """Get operation by idempotency key"""
+        conn = self.get_db()
+        with conn.cursor() as cursor:
+            cursor.execute(
+                "SELECT * FROM outbox_operations WHERE idempotency_key = %s",
+                (idempotency_key,)
+            )
+            row = cursor.fetchone()
+            if row:
+                return self._row_to_operation(row)
+        return None
+    
+    def get_pending_operations(self, limit: int = 100) -> List[OutboxOperation]:
+        """Get pending operations ready for processing"""
+        conn = self.get_db()
+        with conn.cursor() as cursor:
+            cursor.execute("""
+                SELECT * FROM outbox_operations 
+                WHERE status = 'pending' 
+                   OR (status = 'failed' AND retry_count < max_retries 
+                       AND (next_retry_at IS NULL OR next_retry_at <= %s))
+                ORDER BY created_at
+                LIMIT %s
+            """, (datetime.now(timezone.utc), limit))
+            
+            return [self._row_to_operation(row) for row in cursor.fetchall()]
+    
+    def mark_processing(self, operation_id: str) -> bool:
+        """Mark operation as processing"""
+        return self._update_status(operation_id, OutboxStatus.PROCESSING)
+    
+    def mark_completed(self, operation_id: str) -> bool:
+        """Mark operation as completed"""
+        return self._update_status(operation_id, OutboxStatus.COMPLETED)
+    
+    def mark_failed(
+        self,
+        operation_id: str,
+        error_message: str,
+        retry_delay_seconds: int = 60
+    ) -> bool:
+        """Mark operation as failed and schedule retry"""
+        conn = self.get_db()
+        try:
+            with conn.cursor() as cursor:
+                # Get current retry count
+                cursor.execute(
+                    "SELECT retry_count, max_retries FROM outbox_operations WHERE id = %s",
+                    (operation_id,)
+                )
+                row = cursor.fetchone()
+                if not row:
+                    return False
+                
+                retry_count, max_retries = row
+                new_retry_count = retry_count + 1
+                
+                if new_retry_count >= max_retries:
+                    # Max retries reached, mark as permanently failed
+                    status = OutboxStatus.FAILED
+                    next_retry_at = None
+                else:
+                    # Schedule retry
+                    status = OutboxStatus.FAILED
+                    next_retry_at = datetime.now(timezone.utc) + timedelta(seconds=retry_delay_seconds)
+                
+                cursor.execute("""
+                    UPDATE outbox_operations 
+                    SET status = %s, retry_count = %s, error_message = %s, 
+                        next_retry_at = %s, updated_at = %s
+                    WHERE id = %s
+                """, (
+                    status.value, new_retry_count, error_message,
+                    next_retry_at, datetime.now(timezone.utc), operation_id
+                ))
+                
+                return cursor.rowcount > 0
+        except Exception as e:
+            logger.error(f"Failed to mark operation as failed: {e}")
+            return False
+    
+    def _update_status(self, operation_id: str, status: OutboxStatus) -> bool:
+        """Update operation status"""
+        conn = self.get_db()
+        try:
+            with conn.cursor() as cursor:
+                cursor.execute("""
+                    UPDATE outbox_operations 
+                    SET status = %s, updated_at = %s
+                    WHERE id = %s
+                """, (status.value, datetime.now(timezone.utc), operation_id))
+                return cursor.rowcount > 0
+        except Exception as e:
+            logger.error(f"Failed to update operation status: {e}")
+            return False
+    
+    def _row_to_operation(self, row) -> OutboxOperation:
+        """Convert database row to OutboxOperation"""
+        columns = [
+            'id', 'operation_type', 'provider', 'endpoint', 'payload',
+            'idempotency_key', 'status', 'retry_count', 'max_retries',
+            'created_at', 'updated_at', 'next_retry_at', 'error_message', 'metadata'
+        ]
+        data = dict(zip(columns, row))
+        return OutboxOperation.from_dict(data)
+    
+    def cleanup_completed(self, older_than_days: int = 7) -> int:
+        """Clean up completed operations older than specified days"""
+        cutoff = datetime.now(timezone.utc) - timedelta(days=older_than_days)
+        conn = self.get_db()
+        
+        try:
+            with conn.cursor() as cursor:
+                cursor.execute("""
+                    DELETE FROM outbox_operations 
+                    WHERE status = 'completed' AND updated_at < %s
+                """, (cutoff,))
+                return cursor.rowcount
+        except Exception as e:
+            logger.error(f"Failed to cleanup completed operations: {e}")
+            return 0
+    
+    def get_stats(self) -> Dict[str, int]:
+        """Get outbox statistics"""
+        conn = self.get_db()
+        with conn.cursor() as cursor:
+            cursor.execute("""
+                SELECT status, COUNT(*) 
+                FROM outbox_operations 
+                GROUP BY status
+            """)
+            
+            stats = {}
+            for status, count in cursor.fetchall():
+                stats[status] = count
+            
+            return stats
+
+class OutboxProcessor:
+    """Processes outbox operations"""
+    
+    def __init__(self, outbox_manager: OutboxManager, provider_registry: Dict[str, Any]):
+        self.outbox = outbox_manager
+        self.providers = provider_registry
+        self.processing = False
+    
+    async def process_pending(self, batch_size: int = 50) -> Dict[str, int]:
+        """Process pending outbox operations"""
+        if self.processing:
+            logger.warning("Outbox processing already in progress")
+            return {"skipped": 1}
+        
+        self.processing = True
+        stats = {"processed": 0, "completed": 0, "failed": 0, "skipped": 0}
+        
+        try:
+            operations = self.outbox.get_pending_operations(limit=batch_size)
+            
+            for operation in operations:
+                try:
+                    await self._process_operation(operation)
+                    stats["processed"] += 1
+                    stats["completed"] += 1
+                except Exception as e:
+                    logger.error(f"Failed to process operation {operation.id}: {e}")
+                    self.outbox.mark_failed(operation.id, str(e))
+                    stats["processed"] += 1 
+                    stats["failed"] += 1
+        
+        finally:
+            self.processing = False
+        
+        return stats
+    
+    async def _process_operation(self, operation: OutboxOperation):
+        """Process a single outbox operation"""
+        # Mark as processing
+        self.outbox.mark_processing(operation.id)
+        
+        # Get provider
+        provider = self.providers.get(operation.provider)
+        if not provider:
+            raise ValueError(f"Unknown provider: {operation.provider}")
+        
+        # Execute operation based on type
+        if operation.operation_type == "create_task":
+            await provider.create_task_from_payload(operation.payload)
+        elif operation.operation_type == "update_task":
+            await provider.update_task_from_payload(operation.payload)
+        elif operation.operation_type == "delete_task":
+            await provider.delete_task_from_payload(operation.payload)
+        else:
+            raise ValueError(f"Unknown operation type: {operation.operation_type}")
+        
+        # Mark as completed
+        self.outbox.mark_completed(operation.id)
+
+# Helper functions for common operations
+def create_task_operation(
+    provider: str,
+    task_data: Dict[str, Any],
+    outbox: OutboxManager
+) -> OutboxOperation:
+    """Helper to create a task creation operation"""
+    return outbox.add_operation(
+        operation_type="create_task",
+        provider=provider,
+        endpoint="/tasks",
+        payload=task_data,
+        metadata={"task_id": task_data.get("id")}
+    )
+
+def update_task_operation(
+    provider: str,
+    task_id: str,
+    updates: Dict[str, Any],
+    outbox: OutboxManager
+) -> OutboxOperation:
+    """Helper to create a task update operation"""
+    return outbox.add_operation(
+        operation_type="update_task",
+        provider=provider,
+        endpoint=f"/tasks/{task_id}",
+        payload={"task_id": task_id, "updates": updates},
+        metadata={"task_id": task_id}
+    )
\ No newline at end of file
diff --git a/app/utils/retry.py b/app/utils/retry.py
new file mode 100644
index 0000000..ba22b91
--- /dev/null
+++ b/app/utils/retry.py
@@ -0,0 +1,227 @@
+"""
+Exponential backoff with jitter for reliable API calls
+"""
+
+import asyncio
+import random
+import time
+import logging
+from typing import Callable, Any, Optional
+from functools import wraps
+
+logger = logging.getLogger(__name__)
+
+class RetryConfig:
+    """Configuration for retry behavior"""
+    def __init__(
+        self,
+        max_attempts: int = 5,
+        base_delay: float = 1.0,
+        max_delay: float = 60.0,
+        jitter: bool = True,
+        backoff_multiplier: float = 2.0,
+        retryable_exceptions: tuple = (Exception,)
+    ):
+        self.max_attempts = max_attempts
+        self.base_delay = base_delay
+        self.max_delay = max_delay
+        self.jitter = jitter
+        self.backoff_multiplier = backoff_multiplier
+        self.retryable_exceptions = retryable_exceptions
+
+def calculate_delay(attempt: int, config: RetryConfig) -> float:
+    """Calculate exponential backoff delay with optional jitter"""
+    # Exponential backoff: base_delay * multiplier^attempt
+    delay = config.base_delay * (config.backoff_multiplier ** attempt)
+    
+    # Cap at max_delay
+    delay = min(delay, config.max_delay)
+    
+    # Add jitter to prevent thundering herd
+    if config.jitter:
+        # Add ±25% jitter
+        jitter_range = delay * 0.25
+        delay += random.uniform(-jitter_range, jitter_range)
+    
+    return max(0.1, delay)  # Minimum 100ms delay
+
+def retry_with_backoff(config: Optional[RetryConfig] = None):
+    """Decorator for adding exponential backoff retry to functions"""
+    if config is None:
+        config = RetryConfig()
+    
+    def decorator(func: Callable) -> Callable:
+        @wraps(func)
+        async def async_wrapper(*args, **kwargs) -> Any:
+            last_exception = None
+            
+            for attempt in range(config.max_attempts):
+                try:
+                    return await func(*args, **kwargs)
+                except config.retryable_exceptions as e:
+                    last_exception = e
+                    
+                    if attempt == config.max_attempts - 1:
+                        # Last attempt failed, raise the exception
+                        logger.error(f"All {config.max_attempts} attempts failed for {func.__name__}: {e}")
+                        raise
+                    
+                    # Calculate delay and wait
+                    delay = calculate_delay(attempt, config)
+                    logger.warning(f"Attempt {attempt + 1} failed for {func.__name__}: {e}. Retrying in {delay:.2f}s")
+                    
+                    await asyncio.sleep(delay)
+            
+            # This should never be reached due to the raise above
+            raise last_exception
+        
+        @wraps(func)
+        def sync_wrapper(*args, **kwargs) -> Any:
+            last_exception = None
+            
+            for attempt in range(config.max_attempts):
+                try:
+                    return func(*args, **kwargs)
+                except config.retryable_exceptions as e:
+                    last_exception = e
+                    
+                    if attempt == config.max_attempts - 1:
+                        logger.error(f"All {config.max_attempts} attempts failed for {func.__name__}: {e}")
+                        raise
+                    
+                    delay = calculate_delay(attempt, config)
+                    logger.warning(f"Attempt {attempt + 1} failed for {func.__name__}: {e}. Retrying in {delay:.2f}s")
+                    
+                    time.sleep(delay)
+            
+            raise last_exception
+        
+        # Return async or sync wrapper based on function type
+        if asyncio.iscoroutinefunction(func):
+            return async_wrapper
+        else:
+            return sync_wrapper
+    
+    return decorator
+
+# Predefined retry configurations
+HTTP_RETRY_CONFIG = RetryConfig(
+    max_attempts=5,
+    base_delay=1.0,
+    max_delay=60.0,
+    jitter=True,
+    retryable_exceptions=(Exception,)  # Catch all for HTTP - let adapters be specific
+)
+
+DATABASE_RETRY_CONFIG = RetryConfig(
+    max_attempts=3,
+    base_delay=0.5,
+    max_delay=10.0,
+    jitter=True,
+    retryable_exceptions=(Exception,)  # Database-specific exceptions would go here
+)
+
+WEBHOOK_RETRY_CONFIG = RetryConfig(
+    max_attempts=3,
+    base_delay=2.0,
+    max_delay=30.0,
+    jitter=True,
+    retryable_exceptions=(Exception,)
+)
+
+class RetryableHTTPError(Exception):
+    """Base class for HTTP errors that should trigger retries"""
+    def __init__(self, status_code: int, message: str):
+        self.status_code = status_code
+        self.message = message
+        super().__init__(f"HTTP {status_code}: {message}")
+
+class RateLimitError(RetryableHTTPError):
+    """429 Rate limit exceeded"""
+    def __init__(self, retry_after: Optional[int] = None):
+        self.retry_after = retry_after
+        super().__init__(429, f"Rate limit exceeded{f', retry after {retry_after}s' if retry_after else ''}")
+
+class ServerError(RetryableHTTPError):
+    """5xx server errors"""
+    pass
+
+class CircuitBreakerError(Exception):
+    """Circuit breaker is open, not attempting request"""
+    pass
+
+class CircuitBreaker:
+    """Simple circuit breaker pattern for failing fast"""
+    
+    def __init__(
+        self,
+        failure_threshold: int = 5,
+        recovery_timeout: float = 60.0,
+        expected_exceptions: tuple = (Exception,)
+    ):
+        self.failure_threshold = failure_threshold
+        self.recovery_timeout = recovery_timeout
+        self.expected_exceptions = expected_exceptions
+        
+        self.failure_count = 0
+        self.last_failure_time = None
+        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN
+    
+    def call(self, func: Callable, *args, **kwargs) -> Any:
+        """Call function through circuit breaker"""
+        
+        if self.state == "OPEN":
+            if time.time() - self.last_failure_time > self.recovery_timeout:
+                self.state = "HALF_OPEN"
+            else:
+                raise CircuitBreakerError(f"Circuit breaker is OPEN, last failure {time.time() - self.last_failure_time:.1f}s ago")
+        
+        try:
+            result = func(*args, **kwargs)
+            # Success resets circuit breaker
+            if self.state == "HALF_OPEN":
+                self.state = "CLOSED"
+                self.failure_count = 0
+            return result
+            
+        except self.expected_exceptions as e:
+            self.failure_count += 1
+            self.last_failure_time = time.time()
+            
+            if self.failure_count >= self.failure_threshold:
+                self.state = "OPEN"
+                logger.warning(f"Circuit breaker OPENED after {self.failure_count} failures")
+            
+            raise
+
+# Usage examples and testing utilities
+def create_http_retry_config(provider_name: str) -> RetryConfig:
+    """Create HTTP retry config for specific provider"""
+    return RetryConfig(
+        max_attempts=5,
+        base_delay=1.0,
+        max_delay=60.0,
+        jitter=True,
+        retryable_exceptions=(RateLimitError, ServerError, ConnectionError, TimeoutError)
+    )
+
+async def test_retry_behavior():
+    """Test function for retry behavior"""
+    attempt_count = 0
+    
+    @retry_with_backoff(RetryConfig(max_attempts=3, base_delay=0.1))
+    async def failing_function():
+        nonlocal attempt_count
+        attempt_count += 1
+        if attempt_count < 3:
+            raise RateLimitError()
+        return "success"
+    
+    result = await failing_function()
+    assert result == "success"
+    assert attempt_count == 3
+    print("✅ Retry test passed")
+
+if __name__ == "__main__":
+    # Run test
+    asyncio.run(test_retry_behavior())
\ No newline at end of file
diff --git a/tests/test_outbox_pattern.py b/tests/test_outbox_pattern.py
new file mode 100644
index 0000000..827f96e
--- /dev/null
+++ b/tests/test_outbox_pattern.py
@@ -0,0 +1,298 @@
+"""
+Tests for outbox pattern implementation
+"""
+
+import json
+import sqlite3
+from datetime import datetime, timezone, timedelta
+from unittest.mock import Mock, AsyncMock
+from app.utils.outbox import (
+    OutboxOperation, 
+    OutboxManager, 
+    OutboxStatus,
+    OutboxProcessor,
+    create_task_operation,
+    update_task_operation
+)
+
+def create_test_db():
+    """Create in-memory SQLite for testing"""
+    conn = sqlite3.connect(":memory:")
+    conn.row_factory = sqlite3.Row  # Enable column access by name
+    return conn
+
+def test_outbox_operation_creation():
+    """Test OutboxOperation creation and serialization"""
+    
+    operation = OutboxOperation(
+        operation_type="create_task",
+        provider="clickup", 
+        endpoint="/tasks",
+        payload={"title": "Test Task", "description": "Test"},
+        max_retries=3
+    )
+    
+    assert operation.operation_type == "create_task"
+    assert operation.provider == "clickup"
+    assert operation.status == OutboxStatus.PENDING
+    assert operation.retry_count == 0
+    assert operation.max_retries == 3
+    assert operation.idempotency_key is not None
+    
+    # Test serialization roundtrip
+    data = operation.to_dict()
+    restored = OutboxOperation.from_dict(data)
+    
+    assert restored.id == operation.id
+    assert restored.idempotency_key == operation.idempotency_key
+    assert restored.status == operation.status
+
+def test_idempotency_key_generation():
+    """Test idempotency keys are consistent and unique"""
+    
+    # Same payload should generate same key
+    payload = {"title": "Test", "id": "123"}
+    
+    op1 = OutboxOperation("create_task", "clickup", "/tasks", payload)
+    op2 = OutboxOperation("create_task", "clickup", "/tasks", payload)
+    
+    assert op1.idempotency_key == op2.idempotency_key
+    
+    # Different payload should generate different key
+    op3 = OutboxOperation("create_task", "clickup", "/tasks", {"title": "Different"})
+    assert op1.idempotency_key != op3.idempotency_key
+
+def test_outbox_manager_initialization():
+    """Test OutboxManager initializes schema correctly"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    # Check table was created
+    cursor = conn.cursor()
+    cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='outbox_operations'")
+    assert cursor.fetchone() is not None
+
+def test_add_operation():
+    """Test adding operations to outbox"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    operation = manager.add_operation(
+        operation_type="create_task",
+        provider="clickup",
+        endpoint="/tasks", 
+        payload={"title": "Test Task"},
+        max_retries=5
+    )
+    
+    assert operation.id is not None
+    assert operation.status == OutboxStatus.PENDING
+    
+    # Verify stored in database
+    cursor = conn.cursor()
+    cursor.execute("SELECT COUNT(*) FROM outbox_operations")
+    count = cursor.fetchone()[0]
+    assert count == 1
+
+def test_idempotency_enforcement():
+    """Test idempotency prevents duplicate operations"""
+    
+    conn = create_test_db() 
+    manager = OutboxManager(lambda: conn)
+    
+    payload = {"title": "Test Task", "id": "123"}
+    
+    # Add same operation twice
+    op1 = manager.add_operation("create_task", "clickup", "/tasks", payload)
+    op2 = manager.add_operation("create_task", "clickup", "/tasks", payload)
+    
+    # Should return existing operation
+    assert op1.idempotency_key == op2.idempotency_key
+    
+    # Only one record in database
+    cursor = conn.cursor()
+    cursor.execute("SELECT COUNT(*) FROM outbox_operations")
+    count = cursor.fetchone()[0]
+    assert count == 1
+
+def test_get_pending_operations():
+    """Test retrieving pending operations"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    # Add pending operation
+    manager.add_operation("create_task", "clickup", "/tasks", {"title": "Task 1"})
+    
+    # Add failed operation ready for retry
+    failed_op = manager.add_operation("update_task", "clickup", "/tasks/123", {"status": "done"})
+    manager.mark_failed(failed_op.id, "Temporary failure")
+    
+    pending = manager.get_pending_operations()
+    assert len(pending) == 2
+
+def test_mark_operations():
+    """Test marking operations with different statuses"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    operation = manager.add_operation("create_task", "clickup", "/tasks", {"title": "Test"})
+    
+    # Test mark processing
+    success = manager.mark_processing(operation.id)
+    assert success
+    
+    # Test mark completed
+    success = manager.mark_completed(operation.id)
+    assert success
+    
+    # Verify status in database
+    cursor = conn.cursor()
+    cursor.execute("SELECT status FROM outbox_operations WHERE id = ?", (operation.id,))
+    status = cursor.fetchone()[0]
+    assert status == "completed"
+
+def test_mark_failed_with_retry():
+    """Test marking failed operations schedules retry"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    operation = manager.add_operation("create_task", "clickup", "/tasks", {"title": "Test"}, max_retries=3)
+    
+    # Mark failed first time
+    success = manager.mark_failed(operation.id, "Network error", retry_delay_seconds=30)
+    assert success
+    
+    # Check retry is scheduled
+    cursor = conn.cursor()
+    cursor.execute("SELECT retry_count, next_retry_at FROM outbox_operations WHERE id = ?", (operation.id,))
+    row = cursor.fetchone()
+    assert row[0] == 1  # retry_count incremented
+    assert row[1] is not None  # next_retry_at set
+
+def test_mark_failed_max_retries():
+    """Test operation marked as permanently failed after max retries"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    operation = manager.add_operation("create_task", "clickup", "/tasks", {"title": "Test"}, max_retries=2)
+    
+    # Fail twice to reach max retries
+    manager.mark_failed(operation.id, "Error 1")
+    manager.mark_failed(operation.id, "Error 2")
+    
+    # Check no more retries scheduled
+    cursor = conn.cursor()
+    cursor.execute("SELECT retry_count, next_retry_at, status FROM outbox_operations WHERE id = ?", (operation.id,))
+    row = cursor.fetchone()
+    assert row[0] == 2  # Max retry count reached
+    assert row[1] is None  # No next retry scheduled
+    assert row[2] == "failed"  # Still marked as failed
+
+def test_cleanup_completed():
+    """Test cleanup of old completed operations"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    # Add completed operation
+    operation = manager.add_operation("create_task", "clickup", "/tasks", {"title": "Old Task"})
+    manager.mark_completed(operation.id)
+    
+    # Manually set old timestamp
+    old_time = datetime.now(timezone.utc) - timedelta(days=10)
+    cursor = conn.cursor()
+    cursor.execute("UPDATE outbox_operations SET updated_at = ? WHERE id = ?", (old_time, operation.id))
+    
+    # Cleanup operations older than 5 days
+    cleaned = manager.cleanup_completed(older_than_days=5)
+    assert cleaned == 1
+    
+    # Verify operation was deleted
+    cursor.execute("SELECT COUNT(*) FROM outbox_operations WHERE id = ?", (operation.id,))
+    assert cursor.fetchone()[0] == 0
+
+def test_get_stats():
+    """Test outbox statistics"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    # Add operations with different statuses
+    op1 = manager.add_operation("create_task", "clickup", "/tasks", {"title": "Task 1"})
+    op2 = manager.add_operation("create_task", "clickup", "/tasks", {"title": "Task 2"})
+    
+    manager.mark_completed(op1.id)
+    manager.mark_failed(op2.id, "Error")
+    
+    stats = manager.get_stats()
+    assert stats.get("completed", 0) == 1
+    assert stats.get("failed", 0) == 1
+
+async def test_outbox_processor():
+    """Test OutboxProcessor processes operations"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    # Mock provider
+    mock_provider = Mock()
+    mock_provider.create_task_from_payload = AsyncMock()
+    
+    provider_registry = {"clickup": mock_provider}
+    processor = OutboxProcessor(manager, provider_registry)
+    
+    # Add operation
+    manager.add_operation("create_task", "clickup", "/tasks", {"title": "Test Task"})
+    
+    # Process operations
+    stats = await processor.process_pending(batch_size=10)
+    
+    assert stats["processed"] == 1
+    assert stats["completed"] == 1
+    
+    # Verify provider was called
+    mock_provider.create_task_from_payload.assert_called_once()
+
+def test_helper_functions():
+    """Test helper functions for common operations"""
+    
+    conn = create_test_db()
+    manager = OutboxManager(lambda: conn)
+    
+    # Test create task operation
+    task_data = {"id": "task_123", "title": "Helper Test"}
+    operation = create_task_operation("clickup", task_data, manager)
+    
+    assert operation.operation_type == "create_task"
+    assert operation.provider == "clickup"
+    assert operation.payload == task_data
+    
+    # Test update task operation  
+    updates = {"status": "completed"}
+    operation = update_task_operation("clickup", "task_123", updates, manager)
+    
+    assert operation.operation_type == "update_task"
+    assert operation.payload["task_id"] == "task_123"
+    assert operation.payload["updates"] == updates
+
+if __name__ == "__main__":
+    # Run tests without pytest for quick verification
+    test_outbox_operation_creation()
+    test_idempotency_key_generation()
+    test_outbox_manager_initialization()
+    test_add_operation()
+    test_idempotency_enforcement()
+    test_get_pending_operations()
+    test_mark_operations()
+    test_mark_failed_with_retry()
+    test_mark_failed_max_retries()
+    test_cleanup_completed()
+    test_get_stats()
+    test_helper_functions()
+    print("✅ All outbox tests passed!")
\ No newline at end of file
diff --git a/tests/test_retry_backoff.py b/tests/test_retry_backoff.py
new file mode 100644
index 0000000..1104d13
--- /dev/null
+++ b/tests/test_retry_backoff.py
@@ -0,0 +1,194 @@
+"""
+Tests for exponential backoff retry mechanism
+"""
+
+import asyncio
+import time
+from unittest.mock import Mock, AsyncMock
+from app.utils.retry import (
+    RetryConfig, 
+    retry_with_backoff, 
+    calculate_delay,
+    RateLimitError,
+    ServerError,
+    CircuitBreaker
+)
+
+def test_calculate_delay():
+    """Test delay calculation with exponential backoff"""
+    config = RetryConfig(base_delay=1.0, backoff_multiplier=2.0, max_delay=60.0, jitter=False)
+    
+    # Test exponential progression
+    assert calculate_delay(0, config) == 1.0  # 1.0 * 2^0
+    assert calculate_delay(1, config) == 2.0  # 1.0 * 2^1
+    assert calculate_delay(2, config) == 4.0  # 1.0 * 2^2
+    assert calculate_delay(3, config) == 8.0  # 1.0 * 2^3
+    
+    # Test max_delay cap
+    assert calculate_delay(10, config) == 60.0  # Should be capped
+
+def test_calculate_delay_with_jitter():
+    """Test delay calculation includes jitter"""
+    config = RetryConfig(base_delay=4.0, jitter=True)
+    
+    delays = [calculate_delay(1, config) for _ in range(10)]
+    
+    # All delays should be different due to jitter
+    assert len(set(delays)) > 1
+    
+    # All delays should be within reasonable range (base ±25%)
+    for delay in delays:
+        assert 6.0 <= delay <= 10.0  # 8.0 ±25%
+
+def test_sync_retry_success_after_failure():
+    """Test sync retry decorator succeeds after initial failures"""
+    
+    call_count = 0
+    
+    @retry_with_backoff(RetryConfig(max_attempts=3, base_delay=0.01))
+    def flaky_function():
+        nonlocal call_count
+        call_count += 1
+        if call_count < 3:
+            raise RateLimitError()
+        return "success"
+    
+    result = flaky_function()
+    assert result == "success"
+    assert call_count == 3
+
+def test_sync_retry_max_attempts():
+    """Test sync retry gives up after max attempts"""
+    
+    @retry_with_backoff(RetryConfig(max_attempts=2, base_delay=0.01))
+    def always_failing():
+        raise ServerError(500, "Server error")
+    
+    try:
+        always_failing()
+        assert False, "Should have raised ServerError"
+    except ServerError:
+        pass  # Expected
+
+async def test_async_retry_success():
+    """Test async retry decorator"""
+    
+    call_count = 0
+    
+    @retry_with_backoff(RetryConfig(max_attempts=3, base_delay=0.01))
+    async def async_flaky():
+        nonlocal call_count
+        call_count += 1
+        if call_count < 2:
+            raise RateLimitError()
+        return "async_success"
+    
+    result = await async_flaky()
+    assert result == "async_success" 
+    assert call_count == 2
+
+def test_retry_respects_exception_types():
+    """Test retry only retries configured exception types"""
+    
+    @retry_with_backoff(RetryConfig(
+        max_attempts=3, 
+        base_delay=0.01,
+        retryable_exceptions=(RateLimitError,)
+    ))
+    def selective_retry():
+        raise ValueError("Not retryable")
+    
+    # ValueError should not be retried
+    try:
+        selective_retry()
+        assert False, "Should have raised ValueError"
+    except ValueError:
+        pass  # Expected
+
+def test_circuit_breaker_opens():
+    """Test circuit breaker opens after failures"""
+    
+    breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=0.1)
+    
+    def failing_func():
+        raise Exception("Always fails")
+    
+    # First 3 calls should fail and open circuit
+    for _ in range(3):
+        try:
+            breaker.call(failing_func)
+            assert False, "Should have raised Exception"
+        except Exception:
+            pass  # Expected
+    
+    # Circuit should now be open
+    assert breaker.state == "OPEN"
+    
+    # Next call should fail fast with CircuitBreakerError
+    from app.utils.retry import CircuitBreakerError
+    try:
+        breaker.call(failing_func)
+        assert False, "Should have raised CircuitBreakerError"
+    except CircuitBreakerError:
+        pass  # Expected
+
+def test_circuit_breaker_recovery():
+    """Test circuit breaker recovers after timeout"""
+    
+    breaker = CircuitBreaker(failure_threshold=2, recovery_timeout=0.01)
+    
+    def failing_then_succeeding():
+        if hasattr(failing_then_succeeding, 'should_fail'):
+            raise Exception("Fail")
+        return "success"
+    
+    # Fail enough to open circuit
+    failing_then_succeeding.should_fail = True
+    for _ in range(2):
+        try:
+            breaker.call(failing_then_succeeding)
+            assert False, "Should have raised Exception"
+        except Exception:
+            pass  # Expected
+    
+    assert breaker.state == "OPEN"
+    
+    # Wait for recovery timeout
+    time.sleep(0.02)
+    
+    # Remove failure condition
+    del failing_then_succeeding.should_fail
+    
+    # Should succeed and close circuit
+    result = breaker.call(failing_then_succeeding)
+    assert result == "success"
+    assert breaker.state == "CLOSED"
+
+def test_rate_limit_error():
+    """Test RateLimitError includes retry_after"""
+    
+    error = RateLimitError(retry_after=30)
+    assert error.status_code == 429
+    assert error.retry_after == 30
+    assert "retry after 30s" in str(error)
+
+def test_server_error():
+    """Test ServerError formatting"""
+    
+    error = ServerError(502, "Bad Gateway")
+    assert error.status_code == 502
+    assert "HTTP 502: Bad Gateway" in str(error)
+
+if __name__ == "__main__":
+    # Run tests without pytest for quick verification
+    test_calculate_delay()
+    test_calculate_delay_with_jitter()
+    test_sync_retry_success_after_failure()
+    test_sync_retry_max_attempts()
+    asyncio.run(test_async_retry_success())
+    test_retry_respects_exception_types()
+    test_circuit_breaker_opens()
+    test_circuit_breaker_recovery()
+    test_rate_limit_error()
+    test_server_error()
+    print("✅ All retry tests passed!")
\ No newline at end of file
diff --git a/tests/test_retry_simple.py b/tests/test_retry_simple.py
new file mode 100644
index 0000000..28ca106
--- /dev/null
+++ b/tests/test_retry_simple.py
@@ -0,0 +1,51 @@
+"""
+Simple retry test to verify basic functionality
+"""
+
+import asyncio
+import sys
+import os
+sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
+
+from app.utils.retry import retry_with_backoff, RetryConfig, RateLimitError
+
+def test_basic_retry():
+    """Test basic retry functionality"""
+    
+    attempts = 0
+    
+    @retry_with_backoff(RetryConfig(max_attempts=3, base_delay=0.01))
+    def sometimes_fails():
+        nonlocal attempts
+        attempts += 1
+        if attempts < 3:
+            raise RateLimitError()
+        return "success"
+    
+    result = sometimes_fails()
+    assert result == "success"
+    assert attempts == 3
+    print("✅ Basic retry test passed")
+
+async def test_async_retry():
+    """Test async retry"""
+    
+    attempts = 0
+    
+    @retry_with_backoff(RetryConfig(max_attempts=2, base_delay=0.01))
+    async def async_function():
+        nonlocal attempts  
+        attempts += 1
+        if attempts < 2:
+            raise RateLimitError()
+        return "async_success"
+    
+    result = await async_function()
+    assert result == "async_success"
+    assert attempts == 2
+    print("✅ Async retry test passed")
+
+if __name__ == "__main__":
+    test_basic_retry()
+    asyncio.run(test_async_retry())
+    print("✅ All simple retry tests passed!")
\ No newline at end of file
